---
layout: post
title:  "Assignment 1"
date:   2015-11-18
categories: nonsense
comments: true
---

###CSS pre-processors

I really dig the idea of pre-compiling css. The biggest draw for me is that I no longer have to repeat my style settings time and time again thanks to variable-setting, nestling and ,simpler auto-calculations. Add to that the ability to separate the styling over multiple files without sacrificing performance and I'm sold!

That said, I'm far too new in this jam to make any deeper evaluations and once i come face to face with more tools and techniques i might reevaluate. The only drawbacks I see so far would be that it introduces another potential source of bugs and errors. And that the end product looks different to the working documents which might complicate maintenance.

###Static site generators

Another welcome addition to the toolbox. When i first heard of Jekyll I though it was limited to create some fancy looking "one-off product pages" and such but it's really quite powerful. The usage would be for any page that don't require dynamic content (blogs, "product pages" and the lik) Again, I feel that I'm far to green to evaluate and compare tools, but I sure like what I've seen so far. The drawback for me would be that it takes more effort to get up and running and depending on the projects size that might not be worth it.

###Robots.txt

Web robots are “programs” that systematically traverse the web and they can be used for a selection of purposes. Potential usage includes indexing web content for search engines, validating pages or more sinister things such as gathering e-mail addresses for spammers.
Knowing this you probably want to keep some robots away from your site and that’s where Robots.txt comes into play - it allows you to set up rules for which robots are allowed access to your site.
For now, my site lack any restrictions which can be achieved either by typing:

> User-agent: *
> Allow:

Or dropping the robots.txt document altogether.

###Humans.txt

While robots.txt serves information to robots, humans.txt is meant to contain some information for the human user. Usually, if present at all, the document would hold some information about the team that's behind the website.

I chose to add some contact information and also the tools I used as I myself sometimes wonder how other sites are made.

###Comments

To add post-comment support I used the suggested Disqus plugin. I have to say I'm happily surprised at how easy it was to implement. I've always though you had to dabble with databases and what not to set up a decent comments feature. Now i know better!


###Open Graph

Social graph, originally made by facebook, have come to be the standard for which site information is shared across social media services. The idea is to give the creator more power over how the linked resource [should be displayed](https://blog.kissmetrics.com/wp-content/uploads/2014/03/Facebook-2014-03-03.jpg). To use it you simply add a selection of *meta-property-og:* tags in the site head. You can chose what information to include but be aware that some platforms requires certain tags.
